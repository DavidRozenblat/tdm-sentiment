{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_parser.tdm_parser import TdmXmlParser\n",
    "from sentiment_model.sentiment_score import TextAnalysis\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging at the start of your script or notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,          # Set logging level to INFO or DEBUG as needed\n",
    "    format='%(asctime)s %(name)s %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    force=True\n",
    "    \n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "#dataset_name_list = ['Newyork20042023']#, 'LosAngelesTimesDavid', 'TheWashingtonPostDavid','ChicagoTribune', 'USATodayDavid']\n",
    "project_path = Path('/home/ec2-user/SageMaker/david/tdm-sentiment/')\n",
    "data_path = project_path / 'data/'\n",
    "corpus_path = Path('/home/ec2-user/SageMaker/data/')#Newyork20042023_realistic_economy_articles\n",
    "corpus_name = 'TheWashingtonPostDavid'  # 'Newyork20042023_realistic_economy_articles'  'ChicagoTribune_realistic_economy_articles'\n",
    "file_names_path = project_path / 'data/file_names' / corpus_name \n",
    "\n",
    "model_name = 'sentiment_model/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model_path = project_path / 'code' / model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_sentiment(file_path, parser, analyzer):\n",
    "    \"\"\"\n",
    "    Modify an XML file by adding sentiment tags.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file to modify.\n",
    "        parser (object): An object that can parse and manipulate XML (should have `get_xml_soup` and `modify_tag` methods).\n",
    "        analyzer (object): An object that can analyze sentiment (should have `analyze_article_sentiment` method).\n",
    "\n",
    "    Returns:\n",
    "        None or any optional value: Returns None if sentiment tag is already set or in case of error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the XML\n",
    "        soup = parser.get_xml_soup(file_path)\n",
    "        texts = parser.get_art_text(soup)\n",
    "\n",
    "        # Check if 'bert_sentiment' tag already exists and skip if appropriate\n",
    "        bert_sentiment = soup.find('bert_sentiment')\n",
    "        if bert_sentiment:\n",
    "            current_value = float(bert_sentiment.text)\n",
    "            if current_value != 0:\n",
    "                #logger.info(f\"File '{file_path}' already has a non-zero bert_sentiment ({current_value}). Skipping.\")\n",
    "                soup = None\n",
    "                return None\n",
    "            else:\n",
    "                logger.info(f\"File '{file_path}' has a bert_sentiment tag of zero. Will update.\")\n",
    "\n",
    "        # Use the pre-loaded analyzer to get sentiment\n",
    "        value = analyzer.analyze_article_sentiment(texts, method='bert')\n",
    "        logger.info(f\"Calculated sentiment for '{file_path}' is {value}.\")\n",
    "\n",
    "        # Modify the XML with the new sentiment tag\n",
    "        soup = parser.modify_tag(soup, value=value, tag_name='bert_sentiment')\n",
    "\n",
    "        # Write the modified XML back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "        logger.info(f\"Successfully wrote updated sentiment to '{file_path}'.\")\n",
    "\n",
    "        # Clean up\n",
    "        soup = None\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        # Logs the exception with traceback\n",
    "        logger.exception(f\"An error occurred while processing '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_file_names_in_chunks(input_file, chunk_size):\n",
    "    with open(f'{input_file}.txt', 'r') as f:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(f, 1):\n",
    "            file_name = line.strip()\n",
    "            if file_name:\n",
    "                chunk.append(file_name)\n",
    "            if i % chunk_size == 0:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/david_py/lib/python3.12/site-packages/transformers/modeling_utils.py:1439: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded successfully from '/home/ec2-user/SageMaker/david/tdm-sentiment/code/sentiment_model/distilbert-base-uncased-finetuned-sst-2-english' on device -1.\n",
      "Processing chunk: 126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56574459d5244279980ea83e7692faaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80e253fc8d5407f8c071b7a111d966e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411dceb70b9f4aabb7f99ae4c7248efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.conda/envs/david_py/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0245c5514cc74d44808e86d9136ea5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e1e8f17fc6484885047b0f93005be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd2f485cd1f4bc7b3ff5f52a7c5b349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c153549ee94c76a3f8dc9cef07cd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fbe16ae87b4fe89f316c9ab8a08fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b422514946b4b8790ecd81171f20db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc676a5369444992beffdea241849e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parser and analyzer once\n",
    "parser = TdmXmlParser()\n",
    "analyzer = TextAnalysis(model_path)  # Preload the model only once here\n",
    "\n",
    "\n",
    "chunk_size = 2000\n",
    "counter = 0\n",
    "for i, file_chunk in enumerate(read_file_names_in_chunks(file_names_path, chunk_size)):\n",
    "    counter += 1\n",
    "    if counter < 126:\n",
    "        continue\n",
    "    print(\"Processing chunk:\", counter)\n",
    "    file_paths = [corpus_path / path for path in file_chunk]\n",
    "    inner_counter = 0\n",
    "    for i in range(0, len(file_paths), 20):\n",
    "        inner_counter += 1\n",
    "        #if inner_counter < 98: #TODO\n",
    "            #continue #TODO\n",
    "        chunk_path = file_paths[i: i+20]\n",
    "    \n",
    "        with tqdm(total=len(chunk_path)) as pbar:\n",
    "            Parallel(n_jobs=-1, backend='loky')(\n",
    "                delayed(modify_sentiment)(path, parser, analyzer) for path in chunk_path\n",
    "            )\n",
    "            pbar.update(len(chunk_path))\n",
    "#['loky', 'multiprocessing', 'sequential', 'threading']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "david_py",
   "language": "python",
   "name": "david_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
