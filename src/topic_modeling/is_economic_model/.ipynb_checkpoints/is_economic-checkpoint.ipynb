{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# Add the 'code' directory to sys.path\n",
    "project_path = Path(\n",
    "    '/home/ec2-user/SageMaker/david/tdm-sentiment/')\n",
    "sys.path.append(str(project_path / 'code'))\n",
    "\n",
    "from tdm_parser.tdm_parser import TdmXmlParser\n",
    "from train_model import EconomicClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def concatenate_text(text_list):\n",
    "    \"\"\"\n",
    "    Concatenates a list of strings into a single string separated by spaces.\n",
    "    \n",
    "    Args:\n",
    "        text_list (list): A list of strings.\n",
    "        \n",
    "    Returns:\n",
    "        str: A single concatenated string.\n",
    "    \"\"\"\n",
    "    if isinstance(text_list, list):\n",
    "        return ' '.join(text_list)\n",
    "    elif isinstance(text_list, str):\n",
    "        return text_list  # Already a string\n",
    "    else:\n",
    "        return \"\"  # Return empty string for non-list, non-string entries\n",
    "    \n",
    "\n",
    "def dataset_into_df(dataset_path, xml_file_list, parser, choose_set):\n",
    "    \"\"\"\n",
    "    Reads XML files from a specified directory, extracts relevant data, and converts it into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the directory containing the dataset of articles in XML format.\n",
    "        xml_file_list (list): A list of XML file names containing the articles.\n",
    "        parser (object): An object with methods to parse XML into soup and extract data into dictionaries.\n",
    "        choose_set (set): A set of sections to include in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing concatenated data from the specified XML files.\n",
    "    \"\"\"\n",
    "    content_list = []  # Initialize a list to store content dictionaries\n",
    "\n",
    "    # Iterate over XML files in the provided list\n",
    "    for file_name in xml_file_list:\n",
    "        file_path = f'{dataset_path}/{file_name}'  # Construct the full file path\n",
    "        soup = parser.get_xml_soup(file_path)  # Parse the XML file into soup\n",
    "        content_dict = parser.get_xml_to_dict(soup)  # Extract content of the file into a dictionary\n",
    "\n",
    "        # Append the dictionary to the list if it meets the section criteria\n",
    "        if content_dict and content_dict['Section'] in choose_set:\n",
    "            content_list.append(content_dict)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(content_list)\n",
    "    # Concatenates a list of strings into a single string\n",
    "    df['Text'] = df['Text'].apply(concatenate_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_file_names_in_chunks(input_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Open a text file and read each row as a file name in chunks.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): The path to the input text file.\n",
    "        chunk_size (int): The number of lines (file names) to read per chunk.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of file names read from the text file, chunk by chunk.\n",
    "    \"\"\"\n",
    "    with open(f'{input_file}.txt', 'r') as f:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(f, 1):\n",
    "            file_name = line.strip()  # Remove any leading/trailing whitespace/newlines\n",
    "            if file_name:  # Check if line is not empty\n",
    "                chunk.append(file_name)\n",
    "            \n",
    "            if i % chunk_size == 0:\n",
    "                yield chunk\n",
    "                chunk = []  # Reset chunk list for the next batch\n",
    "\n",
    "        # Yield any remaining lines in the last chunk\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def save_file_names_to_txt(file_names, output_file):\n",
    "    \"\"\"\n",
    "    Save a list of file names into a text file, appending to the existing content if it exists.\n",
    "    \n",
    "    Args:\n",
    "        file_names (list): A list of file names to save.\n",
    "        output_file (str): The path to the output text file.\n",
    "    \"\"\"\n",
    "    with open(f'{output_file}.txt', 'a') as f:  \n",
    "        for file_name in file_names:\n",
    "            f.write(file_name + '\\n')\n",
    "    return None\n",
    "\n",
    "\n",
    "positive_set = {'money', 'business', 'finance/business', 'business; part b; business desk', 'financial'}\n",
    "negative_set = {'outlook', 'arts & entertainment', 'style', 'movies', 'arts'}#'sports', \n",
    "choose_set = positive_set | negative_set\n",
    "\n",
    "\n",
    "dataset_name_list = ['USATodayDavid', 'ChicagoTribune', 'Newyork20042023', 'TheWashingtonPostDavid', 'LosAngelesTimesDavid']\n",
    "project_path = Path('/home/ec2-user/SageMaker/david/tdm-sentiment/')\n",
    "input_data_path = project_path / 'data'\n",
    "output_data_path = project_path / 'data'\n",
    "\n",
    "data_sample = Path('/home/ec2-user/SageMaker/data/')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all data file names modified according to their data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset_name_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStartupSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m dataset_name_list:\n\u001b[0;32m----> 5\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_sample\u001b[49m \u001b[38;5;241m/\u001b[39m dataset\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Get all files in the directory\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     xml_file_list \u001b[38;5;241m=\u001b[39m [file\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m dataset_path\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mis_file()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# create all_dataset_file_names txt file \n",
    "dataset_file_names_list = []\n",
    "dataset_name_list = ['StartupSentiment']\n",
    "for dataset in dataset_name_list:\n",
    "    dataset_path = data_sample / dataset\n",
    "    # Get all files in the directory\n",
    "    xml_file_list = [file.name for file in dataset_path.iterdir() if file.is_file()]\n",
    "    for file in xml_file_list:\n",
    "        dataset_file_names_list.append(f'{dataset}/{file}')\n",
    "    \n",
    "path = output_data_path / 'all_dataset_file_names.txt'  \n",
    "if not path.exists():\n",
    "    save_file_names_to_txt(dataset_file_names_list, output_data_path / 'all_dataset_file_names')\n",
    "\n",
    "dataset_file_names_list\n",
    "del dataset_file_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data into a data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TdmXmlParser()\n",
    "data_chunks = []\n",
    "# Process each chunk of file names\n",
    "for file_chunk in read_file_names_in_chunks(output_data_path / 'all_dataset_file_names', chunk_size=10**5):\n",
    "    # Sample a maximum of 10,000 files from each chunk or the total number in the chunk if fewer\n",
    "    file_chunk = random.sample(file_chunk, min(10**4, len(file_chunk)))\n",
    "\n",
    "    # Initialize a progress bar\n",
    "    with tqdm(total=len(file_chunk)) as pbar:\n",
    "        # Process the files in the current chunk\n",
    "        df = dataset_into_df(data_sample, file_chunk, parser, choose_set)  \n",
    "        data_chunks.append(df)  # Append the resulting DataFrame to the list\n",
    "\n",
    "        # Update the progress bar after each chunk is processed\n",
    "        pbar.update(len(file_chunk))\n",
    "\n",
    "# Concatenate all DataFrame chunks into one DataFrame after the loop\n",
    "data = pd.concat(data_chunks, ignore_index=True)\n",
    "\n",
    "# Display the first row of the collected DataFrame\n",
    "print(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "classifier = EconomicClassifier(initialize=True)\n",
    "X_test, y_test = classifier.train_classifier(data)\n",
    "classifier.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate_model(X_test, y_test)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation = classifier.evaluate_model(X_test, y_test)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier with the loaded model and vectorizer\n",
    "classifier = EconomicClassifier()\n",
    "article = \"economy is expected to grow by 3% this year.\"\n",
    "result = classifier.is_economic(article)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_david_py",
   "language": "python",
   "name": "conda_david_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
