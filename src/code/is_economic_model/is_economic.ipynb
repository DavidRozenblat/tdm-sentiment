{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01J5N120YYYAZX19RZBB9QHB4C",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm_parser import TdmXmlParser\n",
    "from train_model import EconomicClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def concatenate_text(text_list):\n",
    "    \"\"\"\n",
    "    Concatenates a list of strings into a single string separated by spaces.\n",
    "    \n",
    "    Args:\n",
    "        text_list (list): A list of strings.\n",
    "        \n",
    "    Returns:\n",
    "        str: A single concatenated string.\n",
    "    \"\"\"\n",
    "    if isinstance(text_list, list):\n",
    "        return ' '.join(text_list)\n",
    "    elif isinstance(text_list, str):\n",
    "        return text_list  # Already a string\n",
    "    else:\n",
    "        return \"\"  # Return empty string for non-list, non-string entries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_into_df(dataset_path, xml_file_list, parser, choose_set):\n",
    "    \"\"\"\n",
    "    Reads XML files from a specified directory, extracts relevant data, and converts it into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the directory containing the dataset of articles in XML format.\n",
    "        xml_file_list (list): A list of XML file names containing the articles.\n",
    "        parser (object): An object with methods to parse XML into soup and extract data into dictionaries.\n",
    "        choose_set (set): A set of sections to include in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing concatenated data from the specified XML files.\n",
    "    \"\"\"\n",
    "    content_list = []  # Initialize a list to store content dictionaries\n",
    "\n",
    "    # Iterate over XML files in the provided list\n",
    "    for file_name in xml_file_list:\n",
    "        file_path = f'{dataset_path}/{file_name}'  # Construct the full file path\n",
    "        soup = parser.get_xml_soup(file_path)  # Parse the XML file into soup\n",
    "        content_dict = parser.get_xml_to_dict(soup)  # Extract content of the file into a dictionary\n",
    "\n",
    "        # Append the dictionary to the list if it meets the section criteria\n",
    "        if content_dict and content_dict['Section'] in choose_set:\n",
    "            content_list.append(content_dict)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(content_list)\n",
    "    # Concatenates a list of strings into a single string\n",
    "    df['Text'] = df['Text'].apply(concatenate_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_file_names_in_chunks(input_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Open a text file and read each row as a file name in chunks.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): The path to the input text file.\n",
    "        chunk_size (int): The number of lines (file names) to read per chunk.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of file names read from the text file, chunk by chunk.\n",
    "    \"\"\"\n",
    "    with open(f'{input_file}.txt', 'r') as f:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(f, 1):\n",
    "            file_name = line.strip()  # Remove any leading/trailing whitespace/newlines\n",
    "            if file_name:  # Check if line is not empty\n",
    "                chunk.append(file_name)\n",
    "            \n",
    "            if i % chunk_size == 0:\n",
    "                yield chunk\n",
    "                chunk = []  # Reset chunk list for the next batch\n",
    "\n",
    "        # Yield any remaining lines in the last chunk\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def save_file_names_to_txt(file_names, output_file):\n",
    "    \"\"\"\n",
    "    Save a list of file names into a text file, appending to the existing content if it exists.\n",
    "    \n",
    "    Args:\n",
    "        file_names (list): A list of file names to save.\n",
    "        output_file (str): The path to the output text file.\n",
    "    \"\"\"\n",
    "    with open(f'{output_file}.txt', 'a') as f:  \n",
    "        for file_name in file_names:\n",
    "            f.write(file_name + '\\n')\n",
    " \n",
    "\n",
    "positive_set = {'money', 'business', 'finance/business', 'business; part b; business desk', 'financial'}\n",
    "negative_set = {'outlook', 'arts & entertainment', 'style', 'movies', 'arts'}#'sports', \n",
    "choose_set = positive_set | negative_set\n",
    "\n",
    "#dataset_name_list = \n",
    "#data_path = \n",
    "dataset_name_list = ['TheNewYorkTimes_sample20', 'USAToday_sample20', 'LosAngelesTimes_sample20', 'TheWashingtonPost_sample25']\n",
    "project_path = Path('c:/Users/97253/OneDrive/Documents/work/bank of israel/financial division/yossi/tdm sentiment/tdm-sentiment/') #TODO\n",
    "input_data_path = project_path / 'data/'\n",
    "output_data_path = project_path / 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all data file names modified according to their data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01J7893ZTVDBJNRE10FZ7R2TE9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all_dataset_file_names txt file \n",
    "dataset_file_names_list = []\n",
    "\n",
    "for dataset in dataset_name_list:\n",
    "    dataset_path = input_data_path / 'data_sample' / dataset\n",
    "    # Get all files in the directory\n",
    "    xml_file_list = [file.name for file in dataset_path.iterdir() if file.is_file()]\n",
    "    for file in xml_file_list:\n",
    "        dataset_file_names_list.append(f'{dataset}/{file}')\n",
    "    \n",
    "path = output_data_path / 'all_dataset_file_names.txt'\n",
    "if not path.exists():\n",
    "    save_file_names_to_txt(dataset_file_names_list, output_data_path / 'all_dataset_file_names')\n",
    "\n",
    "dataset_file_names_list\n",
    "del dataset_file_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data into a data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897a094725d49c09b9092f51dd5f2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        GOID            Publisher                                 Title  \\\n",
      "0  894525606  washington post the  it's not your typical in-law problem   \n",
      "\n",
      "         Date Language Page Section                 Type             Edition  \\\n",
      "0  2011-09-29  english  c.3   style  general information  final - every zone   \n",
      "\n",
      "   Tags Company Name Personal Author Last Name Author First Name Lexile Score  \\\n",
      "0  None         None     None              hax           carolyn         1260   \n",
      "\n",
      "                                                Text  \n",
      "0  adapted from a recent online discussion. hi ca...  \n"
     ]
    }
   ],
   "source": [
    "parser = TdmXmlParser()\n",
    "data_chunks = []\n",
    "# Process each chunk of file names\n",
    "for file_chunk in read_file_names_in_chunks(output_data_path / 'all_dataset_file_names', chunk_size=10**5):\n",
    "    # Sample a maximum of 10,000 files from each chunk or the total number in the chunk if fewer\n",
    "    file_chunk = random.sample(file_chunk, min(10**4, len(file_chunk)))\n",
    "\n",
    "    # Initialize a progress bar\n",
    "    with tqdm(total=len(file_chunk)) as pbar:\n",
    "        # Process the files in the current chunk\n",
    "        df = dataset_into_df(input_data_path / 'data_sample/', file_chunk, parser, choose_set)  #TODO\n",
    "        data_chunks.append(df)  # Append the resulting DataFrame to the list\n",
    "\n",
    "        # Update the progress bar after each chunk is processed\n",
    "        pbar.update(len(file_chunk))\n",
    "\n",
    "# Concatenate all DataFrame chunks into one DataFrame after the loop\n",
    "data = pd.concat(data_chunks, ignore_index=True)\n",
    "\n",
    "# Display the first row of the collected DataFrame\n",
    "print(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01J5N5BB7TMY241XSXXCB3VM3A",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "classifier = EconomicClassifier(initialize=True)\n",
    "X_test, y_test = classifier.train_classifier(data)\n",
    "classifier.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01J5N5KW7QA022BJ1QB11NAWEA",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluation = classifier.evaluate_model(X_test, y_test)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01J5NG018PERR4BE3EWJHKJQ90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier with the loaded model and vectorizer\n",
    "classifier = EconomicClassifier()\n",
    "article = \"economy is expected to grow by 3% this year.\"\n",
    "result = classifier.is_economic(article)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
