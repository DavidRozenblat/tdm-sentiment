{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from is_economic_model.train_model import EconomicClassifier  #TODO\n",
    "from is_economic_model.tdm_parser import TdmXmlParser #TODO\n",
    "from sentiment_model.sentiment_score import TextAnalysis #TODO\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "from joblib import Parallel, delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def read_file_names_in_chunks(input_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Open a text file and read each row as a file name in chunks.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): The path to the input text file.\n",
    "        chunk_size (int): The number of lines (file names) to read per chunk.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of file names read from the text file, chunk by chunk.\n",
    "    \"\"\"\n",
    "    with open(f'{input_file}.txt', 'r') as f:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(f, 1):\n",
    "            file_name = line.strip()  # Remove any leading/trailing whitespace/newlines\n",
    "            if file_name:  # Check if line is not empty\n",
    "                chunk.append(file_name)\n",
    "            \n",
    "            if i % chunk_size == 0:\n",
    "                yield chunk\n",
    "                chunk = []  # Reset chunk list for the next batch\n",
    "\n",
    "        # Yield any remaining lines in the last chunk\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def xml_to_df(file_name, parser):\n",
    "    \"\"\"Read the dataset's XML file and add return into a dict.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to folder containing the dataset of articles in XML files.\n",
    "        xml_file_list (list): list of the xml files containing the articles \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the concatenated data from the XML files.\n",
    "    \"\"\"\n",
    "    # Iterate over XML files in the dataset\n",
    "    file_path = f'{data_path + file_name}'\n",
    "    soup = parser.get_xml_soup(file_path)\n",
    "    #print(soup)\n",
    "    PROPERTY_TAGS = ['GOID', 'SortTitle', 'NumericDate',\n",
    "                'DocSection', 'mstar', \n",
    "                'GenSubjTerm', 'is_economic',\n",
    "                'text_blob_sentiment']\n",
    "    \n",
    "    PROPERTY_NAMES = ['GOID', 'Publisher', 'Date', \n",
    "                 'Section', 'Type', \n",
    "                 'Tags', 'is_economic',\n",
    "                 'text_blob_sentiment']\n",
    "\n",
    "    content_dict = parser.get_xml_to_dict(soup, text=False, property_tags=PROPERTY_TAGS, property_names=PROPERTY_NAMES)  # get content of the file into a dict\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "dataset_name_list = ['TheNewYorkTimes_sample20', 'USAToday_sample20', 'LosAngelesTimes_sample20', 'TheWashingtonPost_sample25'] #TODO\n",
    "project_path = Path('c:/Users/97253/OneDrive/Documents/work/bank of israel/financial division/yossi/tdm sentiment/tdm-sentiment/') #TODO\n",
    "data_path = project_path / 'data/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_sentiment(file_path, parser, analyzer):\n",
    "    \"\"\"\n",
    "    Modify an XML file by adding specific tags.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the XML file to be modified.\n",
    "        parser (TdmXmlParser): Parser instance to handle XML operations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = parser.get_xml_soup(file_path) # get soup\n",
    "        texts = parser.get_art_text(soup) # get article text\n",
    "        \n",
    "        # make sure is_economic tag is 1\n",
    "        is_economic_value = int(soup.find('is_economic').text)\n",
    "        if is_economic_value != 1: \n",
    "            print('Houston, we have a problem!')\n",
    "            return None\n",
    "\n",
    "        # modify text_blob_sentiment\n",
    "        value = analyzer.analyze_article_sentiment(texts, method='textblob') \n",
    "        soup = parser.modify_tag(soup, value=value, tag_name='text_blob_sentiment')\n",
    "        # modify bert_sentiment\n",
    "        value = analyzer.analyze_article_sentiment(texts, method='bert') \n",
    "        soup = parser.modify_tag(soup, value=value, tag_name='bert_sentiment')\n",
    "        \n",
    "        # Write the modified XML back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "# add is_economic and text_blob_sentiment tags and write txt file with economic file names \n",
    "parser = TdmXmlParser()\n",
    "classifier = EconomicClassifier()\n",
    "analyzer = TextAnalysis()\n",
    "\n",
    "chunk_size = 100#00 #TODO # Adjust chunk size based on your memory and performance needs\n",
    "# Split the file list into chunks\n",
    "for i, file_chunk in enumerate(read_file_names_in_chunks(data_path / 'economic_dataset_file_names', chunk_size)):\n",
    "    chunk_paths = [data_path / 'data_sample' / file_name for file_name in file_chunk]\n",
    "    #for file_name in file_chunk:\n",
    "        #path = data_path / 'data_sample' / file_name\n",
    "        #print(modify_sentiment(path, parser, analyzer))\n",
    "    with tqdm(total=len(file_chunk)) as pbar:\n",
    "        # Iterate over XML files in the dataset\n",
    "        Parallel(n_jobs=-1, backend='threading')(delayed(modify_sentiment)(path, parser, analyzer) for path in chunk_paths)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(len(file_chunk))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
