{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample document about machine learning and data science. Another example document discussing natural language processing techniques. More text data helps the model learn vocabulary and contextual information. This is the first document. It contains text about machine learning and data science. The second document focuses on natural language processing and machine learning applications. Text analysis is a part of data science that includes extracting keywords using TF-IDF. More documents may discuss different topics such as computer vision, AI, and deep learning.\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "        \"This is a sample document about machine learning and data science.\",\n",
    "        \"Another example document discussing natural language processing techniques.\",\n",
    "        \"More text data helps the model learn vocabulary and contextual information.\",\n",
    "        \"This is the first document. It contains text about machine learning and data science.\",\n",
    "        \"The second document focuses on natural language processing and machine learning applications.\",\n",
    "        \"Text analysis is a part of data science that includes extracting keywords using TF-IDF.\",\n",
    "        \"More documents may discuss different topics such as computer vision, AI, and deep learning.\"\n",
    "    ]\n",
    "a = \" \".join(texts)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01J71598HPTNESSAA0M5GHCEAK",
   "metadata": {},
   "outputs": [],
   "source": [
    "from is_economic_model.train_model import EconomicClassifier  #TODO\n",
    "from is_economic_model.tdm_parser import TdmXmlParser #TODO\n",
    "from sentiment_model.sentiment_score import TextAnalysis #TODO\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def modify_is_economic(dataset_file_name, parser, classifier, analyzer):\n",
    "    \"\"\"\n",
    "    Modify an XML file by adding specific tags.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the XML file to be modified.\n",
    "        parser (TdmXmlParser): Parser instance to handle XML operations.\n",
    "        classifier (EconomicClassifier): Classifier instance to determine economic relevance.\n",
    "    returns: dataset_file_name if text is economic \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return_obg = None # initialize return object\n",
    "        \n",
    "        # Load the XML file\n",
    "        file_path = data_path /'data_sample'/ dataset_file_name #' # get file path\n",
    "        soup = parser.get_xml_soup(file_path) # get soup\n",
    "        text = parser.get_art_text(soup) # get article text\n",
    "\n",
    "        # add is_economic tag \n",
    "        is_economic_value = 1#classifier.is_economic(text) #TODO\n",
    "        soup = parser.modify_tag(soup, value=is_economic_value, tag_name='is_economic') \n",
    "        if is_economic_value == 1:\n",
    "            value = analyzer.text_blob_sentiment(text)\n",
    "            soup = parser.modify_tag(soup, value=value, tag_name='text_blob_sentiment')\n",
    "            return_obg = dataset_file_name # if article is an economic return file name\n",
    "            \n",
    "        # Write the modified XML back to the file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "        \n",
    "        return return_obg\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \n",
    "\n",
    "\n",
    "def save_file_names_to_txt(file_names, output_file):\n",
    "    \"\"\"\n",
    "    Save a list of file names into a text file, appending to the existing content if it exists.\n",
    "    \n",
    "    Args:\n",
    "        file_names (list): A list of file names to save.\n",
    "        output_file (str): The path to the output text file.\n",
    "    \"\"\"\n",
    "    with open(f'{output_file}.txt', 'a') as f:  # Change 'w' to 'a' to append\n",
    "        for file_name in file_names:\n",
    "            f.write(file_name + '\\n')\n",
    "      \n",
    "            \n",
    "def read_file_names_in_chunks(input_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Open a text file and read each row as a file name in chunks.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): The path to the input text file.\n",
    "        chunk_size (int): The number of lines (file names) to read per chunk.\n",
    "        \n",
    "    Yields:\n",
    "        list: A list of file names read from the text file, chunk by chunk.\n",
    "    \"\"\"\n",
    "    with open(f'{input_file}.txt', 'r') as f:\n",
    "        chunk = []\n",
    "        for i, line in enumerate(f, 1):\n",
    "            file_name = line.strip()  # Remove any leading/trailing whitespace/newlines\n",
    "            if file_name:  # Check if line is not empty\n",
    "                chunk.append(file_name)\n",
    "            \n",
    "            if i % chunk_size == 0:\n",
    "                yield chunk\n",
    "                chunk = []  # Reset chunk list for the next batch\n",
    "\n",
    "        # Yield any remaining lines in the last chunk\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "def xml_to_df(file_name, parser):\n",
    "    \"\"\"Read the dataset's XML file and add return into a dict.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to folder containing the dataset of articles in XML files.\n",
    "        xml_file_list (list): list of the xml files containing the articles \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the concatenated data from the XML files.\n",
    "    \"\"\"\n",
    "    # Iterate over XML files in the dataset\n",
    "    file_path = f'{data_path + file_name}'\n",
    "    soup = parser.get_xml_soup(file_path)\n",
    "    #print(soup)\n",
    "    PROPERTY_TAGS = ['GOID', 'SortTitle', 'NumericDate',\n",
    "                'DocSection', 'mstar', \n",
    "                'GenSubjTerm', 'is_economic',\n",
    "                'text_blob_sentiment', 'WordCount']\n",
    "    \n",
    "    PROPERTY_NAMES = ['GOID', 'Publisher', 'Date', \n",
    "                 'Section', 'Type', \n",
    "                 'Tags', 'is_economic',\n",
    "                 'text_blob_sentiment', 'WordCount']\n",
    "\n",
    "    content_dict = parser.get_xml_to_dict(soup, text=False, property_tags=PROPERTY_TAGS, property_names=PROPERTY_NAMES)  # get content of the file into a dict\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "dataset_name_list = ['TheNewYorkTimes_sample20', 'USAToday_sample20', 'LosAngelesTimes_sample20', 'TheWashingtonPost_sample25']\n",
    "project_path = Path('c:/Users/97253/OneDrive/Documents/work/bank of israel/financial division/yossi/tdm sentiment/tdm-sentiment/') #TODO\n",
    "data_path = project_path / 'data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classify articles if economic \n",
    "classify (or modify) if economic\n",
    "if economic:\n",
    "\n",
    "    - add sentiment\n",
    "  \n",
    "    - add economic articles names to txt file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model 'distilbert-base-uncased-finetuned-sst-2-english' loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9ef201814c495dab7e460af062f8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n",
      "Element 'is_economic' already exists. Updating value.\n",
      "Element 'text_blob_sentiment' already exists. Updating value.\n"
     ]
    }
   ],
   "source": [
    "# add is_economic and text_blob_sentiment tags and write txt file with economic file names \n",
    "parser = TdmXmlParser()\n",
    "classifier = EconomicClassifier()\n",
    "analyzer = TextAnalysis()\n",
    "\n",
    "chunk_size = 10000  # Adjust chunk size based on your memory and performance needs\n",
    "# Split the file list into chunks\n",
    "\n",
    "for file_chunk in read_file_names_in_chunks(data_path / 'all_dataset_file_names', chunk_size):\n",
    "    with tqdm(total=len(file_chunk)) as pbar: #T\n",
    "        # Use Parallel to process files in chunks\n",
    "        file_names = [] #TODO\n",
    "        for file_name in file_chunk: #TODO\n",
    "            name = modify_is_economic(file_name, parser, classifier, analyzer) #TODO\n",
    "            if name != None: #TODO\n",
    "                file_names.append(name) #TODO\n",
    "        #file_names = Parallel(n_jobs=-1)(delayed(modify_is_economic)(file_name, parser, classifier, analyzer) for file_name in file_chunk) \n",
    "        #file_names = [file_name for file_name in file_names if file_name]\n",
    "        save_file_names_to_txt(file_names, data_path / 'economic_dataset_file_names') #T\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(len(file_chunk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get economic properties into df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01J719D4QDRC5XPDVNDK9Z57DS",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model 'distilbert-base-uncased-finetuned-sst-2-english' loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c9ffcb82d347259f2edda76baaf325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'WindowsPath' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 589, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\97253\\AppData\\Local\\Temp\\ipykernel_3660\\3274508748.py\", line 99, in xml_to_df\nTypeError: unsupported operand type(s) for +: 'WindowsPath' and 'str'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, file_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(read_file_names_in_chunks(data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meconomic_dataset_file_names\u001b[39m\u001b[38;5;124m'\u001b[39m, chunk_size)):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(file_chunk)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Iterate over XML files in the dataset\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m         content_list \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_to_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         df_chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(content_list)\n\u001b[0;32m     13\u001b[0m         df_chunk\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m /economic_df_output / economic_article_df_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1693\u001b[0m \n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1699\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1734\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1734\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:736\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    730\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\97253\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:754\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'WindowsPath' and 'str'"
     ]
    }
   ],
   "source": [
    "# add is_economic and text_blob_sentiment tags and write txt file with economic file names \n",
    "parser = TdmXmlParser()\n",
    "classifier = EconomicClassifier()\n",
    "analyzer = TextAnalysis()\n",
    "\n",
    "chunk_size = 10000  # Adjust chunk size based on your memory and performance needs\n",
    "# Split the file list into chunks\n",
    "for i, file_chunk in enumerate(read_file_names_in_chunks(data_path / 'economic_dataset_file_names', chunk_size)):\n",
    "    with tqdm(total=len(file_chunk)) as pbar:\n",
    "        # Iterate over XML files in the dataset\n",
    "        content_list = Parallel(n_jobs=-1)(delayed(xml_to_df)(file_name, parser) for file_name in file_chunk)\n",
    "        df_chunk = pd.DataFrame(content_list)\n",
    "        df_chunk.to_csv(f'{data_path} /economic_df_output / economic_article_df_chunk_{i}.csv', index=False)\n",
    "        # Update the progress bar\n",
    "        pbar.update(len(file_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOID</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Date</th>\n",
       "      <th>Section</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tags</th>\n",
       "      <th>is_economic</th>\n",
       "      <th>text_blob_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1034336215</td>\n",
       "      <td>new york times</td>\n",
       "      <td>2012-08-21</td>\n",
       "      <td>a</td>\n",
       "      <td>news</td>\n",
       "      <td>stop &amp; frisk</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1531107677</td>\n",
       "      <td>new york times</td>\n",
       "      <td>2014-05-18</td>\n",
       "      <td>sp</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1620547810</td>\n",
       "      <td>new york times</td>\n",
       "      <td>2014-11-06</td>\n",
       "      <td>d</td>\n",
       "      <td>news</td>\n",
       "      <td>furniture</td>\n",
       "      <td>1</td>\n",
       "      <td>0.235714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1625069727</td>\n",
       "      <td>new york times</td>\n",
       "      <td>2014-11-15</td>\n",
       "      <td>a</td>\n",
       "      <td>news</td>\n",
       "      <td>battleships</td>\n",
       "      <td>1</td>\n",
       "      <td>0.030058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1690306948</td>\n",
       "      <td>new york times</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>a</td>\n",
       "      <td>news</td>\n",
       "      <td>budgets</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>742633964</td>\n",
       "      <td>washington post the</td>\n",
       "      <td>2010-08-13</td>\n",
       "      <td>a section</td>\n",
       "      <td>news</td>\n",
       "      <td>gays &amp; lesbians</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>755900909</td>\n",
       "      <td>washington post the</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>a section</td>\n",
       "      <td>commentary</td>\n",
       "      <td>renovation &amp; restoration</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>893725652</td>\n",
       "      <td>washington post the</td>\n",
       "      <td>2011-09-22</td>\n",
       "      <td>local living</td>\n",
       "      <td>feature</td>\n",
       "      <td>herbicides</td>\n",
       "      <td>1</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>894525606</td>\n",
       "      <td>washington post the</td>\n",
       "      <td>2011-09-29</td>\n",
       "      <td>style</td>\n",
       "      <td>general information</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.109917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>923420398</td>\n",
       "      <td>washington post the</td>\n",
       "      <td>2012-02-26</td>\n",
       "      <td>extras</td>\n",
       "      <td>news</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GOID            Publisher        Date       Section  \\\n",
       "0   1034336215       new york times  2012-08-21             a   \n",
       "1   1531107677       new york times  2014-05-18            sp   \n",
       "2   1620547810       new york times  2014-11-06             d   \n",
       "3   1625069727       new york times  2014-11-15             a   \n",
       "4   1690306948       new york times  2015-06-23             a   \n",
       "..         ...                  ...         ...           ...   \n",
       "80   742633964  washington post the  2010-08-13     a section   \n",
       "81   755900909  washington post the  2010-10-01     a section   \n",
       "82   893725652  washington post the  2011-09-22  local living   \n",
       "83   894525606  washington post the  2011-09-29         style   \n",
       "84   923420398  washington post the  2012-02-26        extras   \n",
       "\n",
       "                   Type                      Tags  is_economic  \\\n",
       "0                  news              stop & frisk            1   \n",
       "1                  news                       NaN            1   \n",
       "2                  news                 furniture            1   \n",
       "3                  news               battleships            1   \n",
       "4                  news                   budgets            1   \n",
       "..                  ...                       ...          ...   \n",
       "80                 news           gays & lesbians            1   \n",
       "81           commentary  renovation & restoration            1   \n",
       "82              feature                herbicides            1   \n",
       "83  general information                       NaN            1   \n",
       "84                 news                       NaN            1   \n",
       "\n",
       "    text_blob_sentiment  \n",
       "0              0.048857  \n",
       "1              0.000000  \n",
       "2              0.235714  \n",
       "3              0.030058  \n",
       "4              0.072455  \n",
       "..                  ...  \n",
       "80             0.036379  \n",
       "81             0.082393  \n",
       "82             0.061500  \n",
       "83             0.109917  \n",
       "84             0.152602  \n",
       "\n",
       "[85 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_csv_files(folder_path):\n",
    "    # Use pathlib to find all CSV files in the folder\n",
    "    csv_files = list(Path(folder_path).glob('*.csv'))\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "def concat_csv_files(folder_path):\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = get_all_csv_files(folder_path)\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Loop through the list of CSV files\n",
    "    for file in csv_files:\n",
    "        # Read each CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Folder paths\n",
    "folder_path = data_path / 'economic_df_output'\n",
    "\n",
    "# Combine CSV files from both folders\n",
    "combined_df = concat_csv_files(folder_path)\n",
    "\n",
    "# Drop duplicate rows based on the 'GOID' column\n",
    "combined_df = combined_df.drop_duplicates(subset=['GOID'])\n",
    "\n",
    "# Combine 'International Herald Tribune' and 'New York Times' into one category\n",
    "combined_df['Publisher'] = combined_df['Publisher'].replace(\n",
    "    {'international herald tribune': 'new york times the',\n",
    "     'new york times the': 'new york times the'}\n",
    ")\n",
    "\n",
    "# The combined_df now contains the data from both folders with duplicates removed and publishers combined\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path / 'all_dataset_file_names.txt', 'r') as f:\n",
    "    file_name = iter(f)\n",
    "    file_name = next(file_name).strip()\n",
    " \n",
    "#print(file_name)   \n",
    "    \n",
    "project_path = Path('c:/Users/97253/OneDrive/Documents/work/bank of israel/financial division/yossi/tdm sentiment/tdm-sentiment/') #TODO\n",
    "data_path = project_path / 'data/'\n",
    "\n",
    "path = data_path / 'data_sample' / file_name\n",
    "#print(path)\n",
    "\n",
    "parser = TdmXmlParser()\n",
    "soup = parser.get_xml_soup(path)\n",
    "text1 = parser.get_art_text(soup)\n",
    "\n",
    "import re\n",
    "#for text in text1.split('.'):\n",
    "    #text = re.sub(r'\\(photograph by [^\\)]+\\)', '', text)\n",
    "    # Remove URLs and any other unwanted patterns\n",
    "    #text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    #text = re.sub(r'[^a-z0-9\\s]', '', text)  # Keep only letters, numbers, and spaces\n",
    "        \n",
    "\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TdmXmlParser()\n",
    "classifier = EconomicClassifier()\n",
    "analyzer = TextAnalysis()\n",
    "\n",
    "a = analyzer.bert_sentiment(text1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Date</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>650</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WordCount        Date  page\n",
       "0        500  2021-01-01     1\n",
       "1        750  2021-01-01     1\n",
       "2        600  2021-01-01     2\n",
       "3        800  2021-01-02     1\n",
       "4        650  2021-01-02     2\n",
       "5        700  2021-01-02     2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'WordCount': [500, 750, 600, 800, 650, 700],\n",
    "    'Date': ['2021-01-01', '2021-01-01', '2021-01-01',\n",
    "             '2021-01-02', '2021-01-02', '2021-01-02'],\n",
    "    'page': [1, 1, 2, 1, 2, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of words for each page on each day:\n",
      "         Date  page  WordCount\n",
      "0  2021-01-01     1       1250\n",
      "1  2021-01-01     2        600\n",
      "2  2021-01-02     1        800\n",
      "3  2021-01-02     2       1350\n",
      "\n",
      "Average words per page for each day:\n",
      "   page  AverageWordsPerPage\n",
      "0     1               1025.0\n",
      "1     2                975.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Sum of words for each page on each day\n",
    "page_word_sums = df.groupby(['Date', 'page'])['WordCount'].sum().reset_index()\n",
    "print(\"Sum of words for each page on each day:\")\n",
    "print(page_word_sums)\n",
    "\n",
    "# Step 2: Average words per page for each day\n",
    "average_words_per_page = page_word_sums.groupby('page')['WordCount'].mean().reset_index()\n",
    "average_words_per_page.rename(columns={'WordCount': 'AverageWordsPerPage'}, inplace=True)\n",
    "print(\"\\nAverage words per page for each day:\")\n",
    "print(average_words_per_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordCount</th>\n",
       "      <th>Date</th>\n",
       "      <th>page</th>\n",
       "      <th>Newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>750</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>650</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>Times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WordCount       Date  page Newspaper\n",
       "0        500 2021-01-01     1     Times\n",
       "1        750 2021-01-01     1     Times\n",
       "2        600 2021-01-01     2     Times\n",
       "3        800 2021-01-02     1     Times\n",
       "4        650 2021-01-02     2     Times\n",
       "5        700 2021-01-02     2     Times"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Sample DataFrame for a single newspaper\n",
    "data = {\n",
    "    'WordCount': [500, 750, 600, 800, 650, 700],\n",
    "    'Date': ['2021-01-01', '2021-01-01', '2021-01-01',\n",
    "             '2021-01-02', '2021-01-02', '2021-01-02'],\n",
    "    'page': [1, 1, 2, 1, 2, 2],\n",
    "    'Newspaper': ['Times'] * 6  # Only one newspaper\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure that the 'Date' column is of datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Get the newspaper name\n",
    "newspaper_name = df['Newspaper'].iloc[0]\n",
    "\n",
    "# Step 1: Sum of words for each page and date\n",
    "page_word_sums = df.groupby(['Date', 'page'])['WordCount'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate average words per page\n",
    "average_words_per_page = page_word_sums.groupby('page')['WordCount'].mean().reset_index()\n",
    "average_words_per_page.rename(columns={'WordCount': 'AverageWordsPerPage'}, inplace=True)\n",
    "\n",
    "# Convert page numbers to strings for JSON keys\n",
    "page_size_dict = average_words_per_page.set_index('page')['AverageWordsPerPage'].to_dict()\n",
    "page_size_dict = {str(key): value for key, value in page_size_dict.items()}\n",
    "\n",
    "# Store the dictionary using JSON\n",
    "storage_dir = Path('newspaper_page_sizes')\n",
    "storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "file_path = storage_dir / f\"{newspaper_name}_page_sizes.json\"\n",
    "\n",
    "with file_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(page_size_dict, f, ensure_ascii=False, indent=4)\n",
    "#print(f\"Stored page sizes for {newspaper_name} in {file_path}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>AverageWordsPerPage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  page  AverageWordsPerPage\n",
       "0    1                 1250\n",
       "1    2                 1350"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "\n",
    "# Ensure that the 'Date' column is of datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Function to normalize page identifiers\n",
    "def normalize_page(page):\n",
    "    parts = page.split('.')\n",
    "    if len(parts) == 2:\n",
    "        # Sort parts to ensure consistent ordering\n",
    "        return '.'.join(sorted(parts))\n",
    "    return page\n",
    "\n",
    "# Apply normalization\n",
    "df['Page'] = df['Page'].apply(normalize_page)\n",
    "\n",
    "# Step 1: Sum of words for each page and date\n",
    "page_word_sums = df.groupby(['Date', 'Page'])['WordCount'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate average words per page\n",
    "average_words_per_page = page_word_sums.groupby('Page')['WordCount'].mean().reset_index()\n",
    "average_words_per_page.rename(columns={'WordCount': 'AverageWordsPerPage'}, inplace=True)\n",
    "\n",
    "# Convert page numbers to strings for JSON keys\n",
    "page_size_dict = average_words_per_page.set_index('Page')['AverageWordsPerPage'].to_dict()\n",
    "page_size_dict = {str(key): value for key, value in page_size_dict.items()}\n",
    "\n",
    "#storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "file_path = data_path / f\"newspaper_page_sizes/{data_set}.json\" #TODO\n",
    "\n",
    "with file_path.open('w', encoding='utf-8') as f:\n",
    "    json.dump(page_size_dict, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Stored page sizes for {newspaper_name} in {file_path}\")\n",
    "page_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_size_cas(dataset_name, page_tag, data_path):\n",
    "    \"\"\"\n",
    "    Retrieves the page size from the JSON file based on newspaper and page_tag.\n",
    "    \n",
    "    Args:\n",
    "        newspaper (str): Name of the newspaper.\n",
    "        page_tag (str): Normalized page identifier.\n",
    "        storage_dir (str): Directory where JSON files are stored.\n",
    "    \n",
    "    Returns:\n",
    "        int: Page size or None if not found.\n",
    "    \"\"\"\n",
    "    file_path = data_path / f\"newspaper_page_sizes/{dataset_name}.json\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Page sizes file not found for newspaper: {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        page_data = json.load(f)\n",
    "    \n",
    "    page_info = page_data.get(page_tag)\n",
    "    if page_info:\n",
    "        return page_info.get('AverageWordsPerPage', None)\n",
    "    else:\n",
    "        print(f\"Page '{page_tag}' not found in {file_path}\")\n",
    "        return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
