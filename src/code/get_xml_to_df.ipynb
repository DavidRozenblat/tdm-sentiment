{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01J30Z0SWXC8B9C5ZRE49Q6JF5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xml to df helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01J30Z0SWY77D69RVSDZ8Z6F91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_soup(file_path):\n",
    "    \"\"\"Get the root element of an XML file into a soup object.\n",
    "\n",
    "    Args:   \n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        Element: the soup element of the XML file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        xml_data = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    record = soup.find('RECORD')\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_article_text(soup):\n",
    "    \"\"\"get a soup object and return the a list of paragraphs.\n",
    "\n",
    "    Args:\n",
    "        record (a soup object with an HTML parser): The soup object to be parsed.\n",
    "\n",
    "    Returns:\n",
    "        _str_: a string of the text.\n",
    "    \"\"\"\n",
    "    text = soup.find('Text').get_text() # Get the text content of the XML file\n",
    "    if text is None:\n",
    "        return None\n",
    "    html_soup = BeautifulSoup(text, 'html.parser') # Parse the text content as HTML\n",
    "    \n",
    "    paragraphs = html_soup.find_all('p') # Find all paragraph tags in the HTML content\n",
    "    paragraphs_str = '' # Initialize an empty string to store the paragraphs\n",
    "    for p in paragraphs:\n",
    "        if '@' in p.text:\n",
    "            if len(p.text.split(' ')) > 2:\n",
    "                if not 'Credit:' in p.text:\n",
    "                    paragraphs_str += p.text.strip().lower()\n",
    "            \n",
    "    return paragraphs_str\n",
    "\n",
    "\n",
    "def get_xml_to_dict(file_path, property_list, property_names):\n",
    "    \"\"\"Parse an XML file and return a dictionary of its contents.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "    \"\"\"\n",
    "    record = get_xml_soup(file_path)  # Get the root element of the XML file\n",
    "    contect_dict = {} # Initialize an empty dictionary to store the details\n",
    "    \n",
    "    # Iterate over the properties to extract\n",
    "    for i in range(len(property_names)):\n",
    "        prop = record.find(property_list[i])\n",
    "        if prop is not None:\n",
    "            contect_dict[property_names[i]] = prop.text.strip().lower()\n",
    "        else:\n",
    "            contect_dict[property_names[i]] = None\n",
    "            #print(f\"Property {property_list[i]} not found\")\n",
    "\n",
    "    text = get_article_text(record)\n",
    "    if text:\n",
    "        contect_dict['Text'] = text\n",
    "            \n",
    "    return contect_dict\n",
    "\n",
    "\n",
    "def get_properties():\n",
    "    \"\"\"Get the properties to extract from the XML files.\n",
    "\n",
    "    Returns:\n",
    "        _tuple_: _return a tuple of two list of propertiy names and tags_\n",
    "    \"\"\"\n",
    "    property_tags = ['GOID', 'SortTitle','Title', \n",
    "                    'NumericDate', 'Language', \n",
    "                    'StartPage', 'DocSection', \n",
    "                    'mstar', 'DocEdition', 'GenSubjTerm', \n",
    "                    'CompanyName', 'Personal',   \n",
    "                    'LastName', 'FirstName', \n",
    "                    'LexileScore',\n",
    "                    ]\n",
    "    \n",
    "    property_names = ['GOID', 'Publisher', 'Title', \n",
    "                  'Date', 'Language', \n",
    "                  'Page', 'Section', \n",
    "                  'Type', 'Edition', 'Tags',\n",
    "                  'Company Name', 'Personal',\n",
    "                  'Author Last Name', 'Author First Name', \n",
    "                  'Lexile Score', \n",
    "                  ]\n",
    "\n",
    "    return property_tags, property_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01J5MYGQHW8BV7WH36RSX06C2H",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_economic_article(dict):\n",
    "    \"\"\"_classify if article is economic using topic analysis_\n",
    "\n",
    "    Args:\n",
    "        dict (_dict_): _article content_\n",
    "    Returns:\n",
    "        _bool_: _return True if article is economic_\n",
    "    \"\"\"\n",
    "    return True # TODO: Implement this function\n",
    "\n",
    "\n",
    "def get_article_sentiment(paragraph_list):\n",
    "    \"\"\"_get the sentiment of a list of paragraphs_\n",
    "\n",
    "    Args:\n",
    "        paragraph_list (_list_): _a list of paragraphs_\n",
    "    Returns:\n",
    "        _float__: _the sentiment of the paragraphs, a number between -1 and 1_\n",
    "    \"\"\"\n",
    "    # genarate a rendom number between -1 and 1\n",
    "    sentimet_score = random.uniform(-1, 1)\n",
    "    return sentimet_score # TODO: Implement this function\n",
    "\n",
    "\n",
    "def get_article_weight(dict):\n",
    "    \"\"\"_get the weight of an article_\n",
    "\n",
    "    Args:\n",
    "        dict (_dict_): _article content_\n",
    "    Returns:\n",
    "        _float_: _the weight of the article_\n",
    "    \"\"\"\n",
    "    return 1 # TODO: Implement this function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01J30Z0SWY0NVQK853TXBQJNPW",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m content_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  get_article_weight(content_dict) \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# add sentiment analysis to the content_dict\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m content_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_article_sentiment(\u001b[43mcontent_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# add the content_dict to the content_list\u001b[39;00m\n\u001b[0;32m     41\u001b[0m content_list\u001b[38;5;241m.\u001b[39mappend(content_dict)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "# choose the dataset to work with\n",
    "\n",
    "#TheNewYorkTimes_sample20\n",
    "#USAToday_sample20\n",
    "#LosAngelesTimes_sample20\n",
    "#TheWashingtonPost_sample25\n",
    "\n",
    "dataset_name_list = ['TheNewYorkTimes_sample20', 'USAToday_sample20', 'LosAngelesTimes_sample20', 'TheWashingtonPost_sample25']\n",
    "project_path = Path(\"c:/Users/pc/Documents/work/bank of israel/financial division/yossi/tdm-sentiment\")\n",
    "data_path = project_path / 'data'\n",
    "corpus_name = 'LosAngelesTimes_sample20'\n",
    "\n",
    "# get list of files in the data path\n",
    "corpus_path = data_path / 'corpuses' / corpus_name\n",
    "file_name_list = file_list = [f.name for f in corpus_path.glob('*.xml')]\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the content dictionaries\n",
    "content_list = []\n",
    "non_economic_articles = []\n",
    "\n",
    "# get the properties to extract\n",
    "property_tags, property_names = get_properties()\n",
    "\n",
    "# Iterate over the list of file names\n",
    "for file_name in file_name_list:\n",
    "    file_path = corpus_path / file_name\n",
    "    content_dict = get_xml_to_dict(file_path, property_tags, property_names) # get content of the file\n",
    "    \n",
    "    # check if the article is an economic article\n",
    "    economic = is_economic_article(content_dict)\n",
    "    if not economic:\n",
    "        non_economic_articles.append(file_name)\n",
    "        continue \n",
    "    \n",
    "    # add the content_dict to the content_list\n",
    "    content_list.append(content_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01J5MYZ9RTGCWS0G6V4ZW7CT0F",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmls_to_df(file_name_list, property_tags, property_names):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_name_list (list): _a list of file names_\n",
    "        property_tags (list): _description_\n",
    "        property_names (_type_): _description_\n",
    "    return: a df \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01J30Z0SWZXE4ZJ3QQ4BTAJKPG",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "# get Date column in datetime format from format 'YYYY-MM-DD'\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "#df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d', errors='coerce')\n",
    "# sort the dataframe by Date\n",
    "df = df.sort_values(by='Date')\n",
    "# reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "# save the dataframe to a csv file\n",
    "#df.to_csv(f'{dataset_name}.csv', index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01J30Z0SX0J46FRAVEJADWBA03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj\n",
      "Pagination\n",
      "CompanyNameAtt\n",
      "PrintLocation\n",
      "ZipCode\n",
      "PubHistory\n",
      "CoverImageType\n",
      "LexileScore\n",
      "PublisherName\n",
      "EndDate\n",
      "DateLine\n",
      "PublisherAddress\n",
      "StartIssue\n",
      "ObjectIDs\n",
      "Copyright\n",
      "HasGaps\n",
      "CreatedBy\n",
      "Address1\n",
      "Short\n",
      "ContribPersonName\n",
      "GenSubjTerm\n",
      "ColumnHeader\n",
      "AlphaDate\n",
      "Contributors\n",
      "RawLang\n",
      "CompanyNAIC\n",
      "MSTARLegacyID\n",
      "URL\n",
      "mstar\n",
      "Abstract\n",
      "Personal\n",
      "NumericDate\n",
      "CompanyName\n",
      "publisher\n",
      "CompanyDUNS\n",
      "GroupFrosting\n",
      "Medium\n",
      "Name\n",
      "Title\n",
      "EndIssueDate\n",
      "CoverageRange\n",
      "DOCID\n",
      "TextInfo\n",
      "Term\n",
      "FlexTerm\n",
      "PMID\n",
      "Text\n",
      "Flags\n",
      "NormalizedDisplayForm\n",
      "CompanyDUNSAtt\n",
      "StartDate\n",
      "StartVolume\n",
      "EmbargoDays\n",
      "ObjectTypes\n",
      "Qualifier\n",
      "Languages\n",
      "ISO\n",
      "Subject\n",
      "ObjectID\n",
      "ContentModel\n",
      "ISOExpansion\n",
      "DocEdition\n",
      "History\n",
      "Locator\n",
      "PubFrequencies\n",
      "LastNameAtt\n",
      "AlphaStartDate\n",
      "City\n",
      "PublisherXID\n",
      "Geographic\n",
      "NumericStartDate\n",
      "MpubId\n",
      "Subjects\n",
      "DisplayForm\n",
      "TitleAtt\n",
      "Flag\n",
      "other\n",
      "SortTitle\n",
      "Locators\n",
      "SourceType\n",
      "ISOCode\n",
      "CopyrightData\n",
      "SubTitle\n",
      "MiddleNameAtt\n",
      "GOID\n",
      "MappingVersion\n",
      "AbsText\n",
      "OriginalForm\n",
      "AlphaEndDate\n",
      "JournalCode\n",
      "Language\n",
      "PageCount\n",
      "SubTitleAtt\n",
      "NumericEndDate\n",
      "Author\n",
      "FirstName\n",
      "FirstNameAtt\n",
      "StartPage\n",
      "CompanyNAICAtt\n",
      "DFS\n",
      "ObjectRollupType\n",
      "CompanyTerm\n",
      "OriginalFormAtt\n",
      "ContribPersonNameAtt\n",
      "CurrentTitle\n",
      "Province\n",
      "CatalogNum\n",
      "Country\n",
      "Contributor\n",
      "PCID\n",
      "MiddleName\n",
      "PubFrosting\n",
      "SourceRollupType\n",
      "Terms\n",
      "ProvJournalCode\n",
      "TitleKeyword\n",
      "LegacyDataMapping\n",
      "BrowseType\n",
      "LastName\n",
      "GenSubjValue\n",
      "DocSection\n"
     ]
    }
   ],
   "source": [
    "tags_list = []\n",
    "for file_name in file_name_list:    \n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    soup = get_xml_soup(file_path)\n",
    "\n",
    "    # Initialize an empty set to hold the tag names\n",
    "    \n",
    "    tags = set()\n",
    "\n",
    "    # Iterate through all elements in the soup object\n",
    "    for element in soup.find_all(True):\n",
    "        tags.add(element.name)\n",
    "\n",
    "    # Convert the set to a list (if desired) and add it to the tags_list\n",
    "    tags_list.append(tags)\n",
    "    \n",
    "    #tags_list = tags_list.append(tags))\n",
    "    \n",
    "# Iterate over the list and union each set with the merged_set\n",
    "merged_set = set()\n",
    "for s in tags_list:\n",
    "    merged_set = merged_set.union(s)\n",
    "\n",
    "# Print the merged set\n",
    "#print(merged_set)\n",
    "#print(tags_list)\n",
    "for tags in merged_set:\n",
    "    print(tags)\n",
    "    #print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
